[device]  
device_id = "" 

[dir]
work_space = ../
save_tag = prosody_cbhg_test

[dict]
base_dir = ../data/dict/
word_dict = char.dict
wordvec_dict = word2vec_decompress.feat
token_dict = seg_token.dict
prop_dict = seg_prop.dict
#dyz_dict_path = dyzGB.dict
#dyzpy_dict_path = dyzPY.dict
initial_dict = initial.dict
final_dict = final.dict
tone_dict = tone.dict
prosody_dict = prosody_label.dict
#dyz_list_path = dyzList_150.txt
dyz_py_dict = dyz_split_py_list_150.txt
feature_names = wordvec&token&prop
mark_names = prosody

[train]
initStdv = 0.02
batch_size = 120
dropout_rate = 0.1
init_model = 
learning_rate = 0.0001
optimator = Adam
#loss_type = crf|mce|mse
loss_type = crf&mce
loss_weight = 
#loss_weight = | |
#loss_weight = 0.17|0.17|0.66
start_epoch = 0
epochs = 1
print_freq = 1
train_dataset = adapt
dev_dataset = adapt
eval_metric = EvalProsody

[test]
model_name = model-82
test_dataset = 
output_dir = /home/speech/niezhipeng/dyz_lstm_cbhg/output/cbhg_3*bi-lstm_no_dp/

[train_data_info]
base_dir = ../data/20180319_f11/dev_file
fea_data_name = tag_shuffle_validation_common.feature
tar_data_names = tag_shuffle_validation_common.target
encoding = gbk

[dev_data_info]
base_dir = ../data/20180319_f11/dev_file
fea_data_name = tag_shuffle_validation_common.feature
tar_data_names = tag_shuffle_validation_common.target
encoding = gbk

[adapt_data_info]
base_dir = ../data/20180319_f11/test_file
fea_data_name = tag_shuffle_adapt_test.feature
tar_data_names = tag_shuffle_adapt_test.target
encoding = gbk

[novel_data_info]
base_dir = 
fea_data_name = 
tar_data_names = 
encoding = 

[model]
use_crf = false
struct = BBB
mean_var_path = ../data/prosody_128dim_common_f11.mean_var

[hidden_layer_1]
layer_type = fc
num_units = 128
activation = sigmoid

[hidden_layer_2]
layer_type = cbhg
num_units = 128
kernel_size = 7
proj_nums = 128&128

[hidden_layer_3]
layer_type = rnn
struct = bi
cell_name = GRUCell
num_units = 64
use_peepholes = True
cell_clip = None
initializer = None
num_proj = 128
proj_clip = None
forget_bias = 0
activation = tanh

[hidden_layer_4]
layer_type = fc
num_units = 128
activation = sigmoid
regularizer = l2_loss

[output_layer@task_1]
layer_type = fc
num_units = 64
activation = linear
regularizer = l2_loss

[Adam]
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8
use_locking = False
